{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install Libraries"
      ],
      "metadata": {
        "id": "1mz7LN22Ml80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCkhSGVfLuhG",
        "outputId": "194f82cd-a187-4f0f-960b-546042f6273f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken matplotlib  --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize Packages & setting up the OpenAI Key"
      ],
      "metadata": {
        "id": "WiDRnRQiMrLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define OpenAI API keys\n",
        "\n",
        "import json\n",
        "import openai\n",
        "import os\n",
        "\n",
        "api_key =\"sk-***\"\n",
        "openai.api_key = api_key\n",
        "os.environ['OPENAI_API_KEY'] = api_key"
      ],
      "metadata": {
        "id": "Chnmn-6hL0Fp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Validation Process"
      ],
      "metadata": {
        "id": "Z2axtQbCM_jM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tiktoken\n",
        "# import openai\n",
        "# import json\n",
        "\n",
        "# # Constants\n",
        "# MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "# TARGET_EPOCHS = 2\n",
        "# MIN_TARGET_EXAMPLES = 20\n",
        "# MAX_TARGET_EXAMPLES = 50\n",
        "\n",
        "# # Initialize the token encoding model\n",
        "# encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# # ---- Token Counting Utilities ----\n",
        "\n",
        "# def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "#     \"\"\"\n",
        "#     Calculate the total number of tokens in a list of messages.\n",
        "\n",
        "#     Parameters:\n",
        "#     messages (list): A list of dictionaries containing messages with roles and content.\n",
        "#     tokens_per_message (int): The base number of tokens for each message. Default is 3.\n",
        "#     tokens_per_name (int): Number of tokens for each name in the message. Default is 1.\n",
        "\n",
        "#     Returns:\n",
        "#     int: The total number of tokens for the list of messages.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Initialize the total token count.\n",
        "#     num_tokens = 0\n",
        "\n",
        "#     # Iterate through each message in the list of messages.\n",
        "#     for message in messages:\n",
        "#         # Add the base tokens_per_message to the total count.\n",
        "#         num_tokens += tokens_per_message\n",
        "\n",
        "#         # Iterate through key-value pairs in the message.\n",
        "#         for key, value in message.items():\n",
        "#             # Add the length of the encoded value to the total count.\n",
        "#             num_tokens += len(encoding.encode(value))\n",
        "\n",
        "#             # Check if the key is \"name\" and add tokens_per_name if it is.\n",
        "#             if key == \"name\":\n",
        "#                 num_tokens += tokens_per_name\n",
        "\n",
        "#     # Add 3 additional tokens for message separation.\n",
        "#     num_tokens += 3\n",
        "\n",
        "#     # Return the total number of tokens for the list of messages.\n",
        "#     return num_tokens\n",
        "\n",
        "\n",
        "# def num_assistant_tokens_from_messages(messages):\n",
        "#       \"\"\"\n",
        "#     Calculate the number of tokens in assistant's messages within a list of messages.\n",
        "\n",
        "#     Parameters:\n",
        "#     messages (list): A list of dictionaries containing messages with roles and content.\n",
        "\n",
        "#     Returns:\n",
        "#     int: The number of tokens used in assistant's responses.\n",
        "#     \"\"\"\n",
        "#     # Calculate the number of tokens in assistant messages within a list of messages.\n",
        "#     num_tokens = 0\n",
        "#     for message in messages:\n",
        "#         if message[\"role\"] == \"assistant\":\n",
        "#             num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "#     return num_tokens\n",
        "\n",
        "# def print_distribution(values, name):\n",
        "#     \"\"\"\n",
        "#     Print statistical distribution information for a list of values.\n",
        "\n",
        "#     Parameters:\n",
        "#     values (list): List of numerical values for which distribution is calculated.\n",
        "#     name (str): Descriptive name for the list of values.\n",
        "#     \"\"\"\n",
        "#     # Print distribution statistics for a list of values.\n",
        "#     print(f\"\\n#### Distribution of {name}:\")\n",
        "#     print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "#     print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "#     print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
        "\n",
        "# # ---- Additional Check: Duplicate Conversations ----\n",
        "\n",
        "# def check_for_duplicates(dataset):\n",
        "#     \"\"\"\n",
        "#     Check for duplicate conversations in the dataset.\n",
        "\n",
        "#     Parameters:\n",
        "#     dataset (list): List of conversation data.\n",
        "\n",
        "#     Prints:\n",
        "#     Number of duplicate conversations found in the dataset.\n",
        "#     \"\"\"\n",
        "#     # Check for duplicate conversations in the dataset.\n",
        "#     unique_conversations = set()\n",
        "#     duplicate_count = 0\n",
        "\n",
        "#     for ex in dataset:\n",
        "#         serialized = json.dumps(ex, sort_keys=True)\n",
        "#         if serialized in unique_conversations:\n",
        "#             duplicate_count += 1\n",
        "#         else:\n",
        "#             unique_conversations.add(serialized)\n",
        "\n",
        "#     print(f\"\\n### Duplicate Conversations Check ###\")\n",
        "#     print(f\"Number of duplicate conversations: {duplicate_count}\")\n",
        "\n",
        "# # ---- Data Warnings and Token Counts ----\n",
        "\n",
        "# def analyze_data_and_tokens(dataset):\n",
        "#     \"\"\"\n",
        "#     Perform a comprehensive analysis on the dataset to ensure its quality.\n",
        "\n",
        "#     Parameters:\n",
        "#     dataset (list): List of conversation data.\n",
        "\n",
        "#     Prints:\n",
        "#     Various warnings, token counts, and other statistics for the dataset.\n",
        "#     \"\"\"\n",
        "#     # Analyze the dataset for data warnings and token counts.\n",
        "#     n_missing_system = 0\n",
        "#     n_missing_user = 0\n",
        "#     n_messages = []\n",
        "#     convo_lens = []\n",
        "#     assistant_message_lens = []\n",
        "#     for ex in dataset:\n",
        "#         messages = ex[\"messages\"]\n",
        "#         if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "#             n_missing_system += 1\n",
        "#         if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "#             n_missing_user += 1\n",
        "#         n_messages.append(len(messages))\n",
        "#         convo_lens.append(num_tokens_from_messages(messages))\n",
        "#         assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "#     print(f\"\\n### Data Analysis and Token Counts ###\")\n",
        "#     print(\"Num examples missing system message:\", n_missing_system)\n",
        "#     print(\"Num examples missing user message:\", n_missing_user)\n",
        "#     print_distribution(n_messages, \"num_messages_per_example\")\n",
        "#     print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "#     print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "\n",
        "#     n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "#     print(f\"{n_too_long} examples may be over the 4096 token limit and will be truncated during fine-tuning\")\n",
        "\n",
        "# # ---- Cost Estimation ----\n",
        "\n",
        "# def estimate_cost(dataset, price_per_1k_token=0.0005):\n",
        "#     \"\"\"\n",
        "#     Estimate the cost of fine-tuning the model based on dataset size and token count.\n",
        "\n",
        "#     Parameters:\n",
        "#     dataset (list): List of conversation data.\n",
        "#     price_per_1k_token (float): Cost per 1,000 tokens for training the model.\n",
        "\n",
        "#     Prints:\n",
        "#     Estimated cost for training the model.\n",
        "#     \"\"\"\n",
        "#     # Estimate the cost of training using the dataset.\n",
        "#     n_epochs = TARGET_EPOCHS\n",
        "#     n_train_examples = len(dataset)\n",
        "\n",
        "#     if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "#         n_epochs = min(MAX_TARGET_EXAMPLES, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "#     elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "#         n_epochs = max(MIN_TARGET_EXAMPLES, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "#     convo_lens = [num_tokens_from_messages(ex[\"messages\"]) for ex in dataset]\n",
        "#     n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "\n",
        "#     print(f\"\\n### Cost Estimation ###\")\n",
        "#     print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training.\")\n",
        "#     print(f\"By default, you'll train for {n_epochs} epochs on this dataset.\")\n",
        "#     print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens.\")\n",
        "\n",
        "#     # Cost Estimation\n",
        "#     total_tokens = n_epochs * n_billing_tokens_in_dataset\n",
        "#     estimated_cost = (total_tokens / 1000) * price_per_1k_token\n",
        "\n",
        "#     print(f\"Estimated cost for this training is approximately ${estimated_cost}\")\n",
        "\n",
        "\n",
        "# # ---- Execution Pipeline ----\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     data_path = \"/content/chat_data.jsonl\"\n",
        "\n",
        "#     # Load the dataset\n",
        "#     with open(data_path, 'r', encoding='utf-8') as f:\n",
        "#         dataset = [json.loads(line) for line in f]\n",
        "\n",
        "#     # Phase 2: Token Counting and Cost Estimation\n",
        "#     analyze_data_and_tokens(dataset)\n",
        "#     estimate_cost(dataset)\n",
        "#     check_for_duplicates(dataset)\n"
      ],
      "metadata": {
        "id": "bTwpjlLZ_s5I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tiktoken\n",
        "import openai\n",
        "import json\n",
        "\n",
        "# Constants\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "TARGET_EPOCHS = 2\n",
        "MIN_TARGET_EXAMPLES = 20\n",
        "MAX_TARGET_EXAMPLES = 50\n",
        "\n",
        "# Initialize the token encoding model\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# ---- Token Counting Utilities ----\n",
        "\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    \"\"\"\n",
        "    Calculate the total number of tokens in a list of messages.\n",
        "\n",
        "    Parameters:\n",
        "    messages (list): A list of dictionaries containing messages with roles and content.\n",
        "    tokens_per_message (int): The base number of tokens for each message. Default is 3.\n",
        "    tokens_per_name (int): Number of tokens for each name in the message. Default is 1.\n",
        "\n",
        "    Returns:\n",
        "    int: The total number of tokens for the list of messages.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the total token count.\n",
        "    num_tokens = 0\n",
        "\n",
        "    # Iterate through each message in the list of messages.\n",
        "    for message in messages:\n",
        "        # Add the base tokens_per_message to the total count.\n",
        "        num_tokens += tokens_per_message\n",
        "\n",
        "        # Iterate through key-value pairs in the message.\n",
        "        for key, value in message.items():\n",
        "            # Add the length of the encoded value to the total count.\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "\n",
        "            # Check if the key is \"name\" and add tokens_per_name if it is.\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "\n",
        "    # Add 3 additional tokens for message separation.\n",
        "    num_tokens += 3\n",
        "\n",
        "    # Return the total number of tokens for the list of messages.\n",
        "    return num_tokens\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    \"\"\"\n",
        "    Calculate the number of tokens in assistant's messages within a list of messages.\n",
        "\n",
        "    Parameters:\n",
        "    messages (list): A list of dictionaries containing messages with roles and content.\n",
        "\n",
        "    Returns:\n",
        "    int: The number of tokens used in assistant's responses.\n",
        "    \"\"\"\n",
        "    num_tokens = 0\n",
        "    for message in messages:  # iterate through each message in the list\n",
        "        if message[\"role\"] == \"assistant\":  # if the role of message is 'assistant'\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))  # encode the content and count its length\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    \"\"\"\n",
        "    Print statistical distribution information for a list of values.\n",
        "\n",
        "    Parameters:\n",
        "    values (list): List of numerical values for which distribution is calculated.\n",
        "    name (str): Descriptive name for the list of values.\n",
        "    \"\"\"\n",
        "    # print the statistical distribution of the given 'values'\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")  # print minimum and maximum of the 'values'\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")  # print mean and median of the 'values'\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")  # print 10th and 90th percentiles\n",
        "\n",
        "def check_for_duplicates(dataset):\n",
        "    \"\"\"\n",
        "    Check for duplicate conversations in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (list): List of conversation data.\n",
        "\n",
        "    Prints:\n",
        "    Number of duplicate conversations found in the dataset.\n",
        "    \"\"\"\n",
        "    unique_conversations = set()  # a set to store unique conversations\n",
        "    duplicate_count = 0  # counter for duplicate conversations\n",
        "\n",
        "    for ex in dataset:  # iterate through each example in the dataset\n",
        "        serialized = json.dumps(ex, sort_keys=True)  # serialize the example\n",
        "        if serialized in unique_conversations:  # if the serialized example is already in the set\n",
        "            duplicate_count += 1  # increment the duplicate counter\n",
        "        else:\n",
        "            unique_conversations.add(serialized)  # if not, add it to the set\n",
        "\n",
        "    print(f\"\\n### Duplicate Conversations Check ###\")\n",
        "    print(f\"Number of duplicate conversations: {duplicate_count}\")\n",
        "\n",
        "def analyze_data_and_tokens(dataset):\n",
        "    \"\"\"\n",
        "    Perform a comprehensive analysis on the dataset to ensure its quality.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (list): List of conversation data.\n",
        "\n",
        "    Prints:\n",
        "    Various warnings, token counts, and other statistics for the dataset.\n",
        "    \"\"\"\n",
        "    n_missing_system = 0  # counter for missing system messages\n",
        "    n_missing_user = 0  # counter for missing user messages\n",
        "    n_messages = []  # store number of messages in each conversation\n",
        "    convo_lens = []  # store the total number of tokens in each conversation\n",
        "    assistant_message_lens = []  # store the total number of tokens in assistant messages in each conversation\n",
        "\n",
        "    for ex in dataset:  # iterate through each example in the dataset\n",
        "        messages = ex[\"messages\"]\n",
        "        if not any(message[\"role\"] == \"system\" for message in messages):  # check for missing system message\n",
        "            n_missing_system += 1\n",
        "        if not any(message[\"role\"] == \"user\" for message in messages):  # check for missing user message\n",
        "            n_missing_user += 1\n",
        "        n_messages.append(len(messages))  # append the number of messages\n",
        "        convo_lens.append(num_tokens_from_messages(messages))  # append the total number of tokens\n",
        "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))  # append the total number of tokens in assistant messages\n",
        "\n",
        "    # print the results of the analysis\n",
        "    print(f\"\\n### Data Analysis and Token Counts ###\")\n",
        "    print(\"Num examples missing system message:\", n_missing_system)\n",
        "    print(\"Num examples missing user message:\", n_missing_user)\n",
        "    print_distribution(n_messages, \"num_messages_per_example\")\n",
        "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "\n",
        "    n_too_long = sum(l > 4096 for l in convo_lens)  # count of conversations exceeding the token limit\n",
        "    print(f\"{n_too_long} examples may be over the 4096 token limit and will be truncated during fine-tuning\")\n",
        "\n",
        "def estimate_cost(dataset, price_per_1k_token=0.0005):\n",
        "    \"\"\"\n",
        "    Estimate the cost of fine-tuning the model based on dataset size and token count.\n",
        "\n",
        "    Parameters:\n",
        "    dataset (list): List of conversation data.\n",
        "    price_per_1k_token (float): Cost per 1,000 tokens for training the model.\n",
        "\n",
        "    Prints:\n",
        "    Estimated cost for training the model.\n",
        "    \"\"\"\n",
        "    n_epochs = TARGET_EPOCHS  # target number of epochs for training\n",
        "    n_train_examples = len(dataset)  # total number of examples in the dataset\n",
        "\n",
        "    # adjust the number of epochs based on the number of training examples\n",
        "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "        n_epochs = min(MAX_TARGET_EXAMPLES, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "        n_epochs = max(MIN_TARGET_EXAMPLES, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "    convo_lens = [num_tokens_from_messages(ex[\"messages\"]) for ex in dataset]  # total number of tokens in each conversation\n",
        "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)  # total number of tokens to be billed\n",
        "\n",
        "    # print the cost estimation\n",
        "    print(f\"\\n### Cost Estimation ###\")\n",
        "    print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training.\")\n",
        "    print(f\"By default, you'll train for {n_epochs} epochs on this dataset.\")\n",
        "    print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens.\")\n",
        "\n",
        "    total_tokens = n_epochs * n_billing_tokens_in_dataset  # total number of tokens to be billed\n",
        "    estimated_cost = (total_tokens / 1000) * price_per_1k_token  # calculate the estimated cost\n",
        "\n",
        "    print(f\"Estimated cost for this training is approximately ${estimated_cost}\")\n",
        "\n",
        "# ---- Execution Pipeline ----\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/chat_data.jsonl\"\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        dataset = [json.loads(line) for line in f]\n",
        "\n",
        "    # Phase 2: Token Counting and Cost Estimation\n",
        "    analyze_data_and_tokens(dataset)\n",
        "    estimate_cost(dataset)\n",
        "    check_for_duplicates(dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mIPPwOUQj3D",
        "outputId": "ea9a84ea-de16-42dc-e106-fc45534d15d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### Data Analysis and Token Counts ###\n",
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 50, 79\n",
            "mean / median: 66.65, 68.0\n",
            "p5 / p95: 58.5, 74.2\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 14, 28\n",
            "mean / median: 22.85, 24.0\n",
            "p5 / p95: 17.6, 26.1\n",
            "0 examples may be over the 4096 token limit and will be truncated during fine-tuning\n",
            "\n",
            "### Cost Estimation ###\n",
            "Dataset has ~1333 tokens that will be charged for during training.\n",
            "By default, you'll train for 2 epochs on this dataset.\n",
            "By default, you'll be charged for ~2666 tokens.\n",
            "Estimated cost for this training is approximately $0.001333\n",
            "\n",
            "### Duplicate Conversations Check ###\n",
            "Number of duplicate conversations: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload Chat Data"
      ],
      "metadata": {
        "id": "Ef2OpKSpNFjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the file you want to upload\n",
        "file_name = \"/content/chat_data.jsonl\"\n",
        "\n",
        "# Use OpenAI's File.create method to upload the file for the purpose of fine-tuning\n",
        "# The file is opened in binary read mode (\"rb\")\n",
        "upload_response = openai.File.create(\n",
        "  file=open(file_name, \"rb\"),  # Open the file in binary read mode\n",
        "  purpose='fine-tune'  # Specify that the file is intended for fine-tuning a model\n",
        ")\n",
        "\n",
        "# Output the upload response to see the details of the uploaded file\n",
        "upload_response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m-3_8puHwLy",
        "outputId": "883566a9-02d4-41b4-84ff-fc7f493c3854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<File file id=file-xsa036zTL32wxy5eGSUzjwpE at 0x7d7ec0ccb6f0> JSON: {\n",
              "  \"object\": \"file\",\n",
              "  \"id\": \"file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "  \"purpose\": \"fine-tune\",\n",
              "  \"filename\": \"file\",\n",
              "  \"bytes\": 8474,\n",
              "  \"created_at\": 1695643341,\n",
              "  \"status\": \"uploaded\",\n",
              "  \"status_details\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Identify uploaded file id"
      ],
      "metadata": {
        "id": "AKZgivzONL2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save file name\n",
        "file_id = upload_response.id\n",
        "file_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lB5_l5ngJ9S4",
        "outputId": "8d32b4ff-7d74-4ca6-b143-16dbccfef723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'file-xsa036zTL32wxy5eGSUzjwpE'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initiate the finetuning job"
      ],
      "metadata": {
        "id": "z8n6g74VNRuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the FineTuningJob.create method to initiate the fine-tuning process\n",
        "# Provide the training_file using the previously saved file_id and specify the model to use\n",
        "openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faQZaLtLKCUt",
        "outputId": "a8d0dac4-cab3-4df5-822e-ce15ead2c27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<FineTuningJob fine_tuning.job id=ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn at 0x7d7ec0d199e0> JSON: {\n",
              "  \"object\": \"fine_tuning.job\",\n",
              "  \"id\": \"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"created_at\": 1695643358,\n",
              "  \"finished_at\": null,\n",
              "  \"fine_tuned_model\": null,\n",
              "  \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "  \"result_files\": [],\n",
              "  \"status\": \"validating_files\",\n",
              "  \"validation_file\": null,\n",
              "  \"training_file\": \"file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "  \"hyperparameters\": {\n",
              "    \"n_epochs\": \"auto\"\n",
              "  },\n",
              "  \"trained_tokens\": null,\n",
              "  \"error\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieve finetuning jobs for verification"
      ],
      "metadata": {
        "id": "V8Jtrnk0NXcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List 10 fine-tuning jobs\n",
        "openai.FineTuningJob.list(limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYdZZ-oQKdUz",
        "outputId": "8b90d794-1ccf-4c4e-989b-10b74ade1e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7d7e8bdd7b00> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"finished_at\": null,\n",
              "      \"fine_tuned_model\": null,\n",
              "      \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "      \"result_files\": [],\n",
              "      \"status\": \"running\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 5\n",
              "      },\n",
              "      \"trained_tokens\": null,\n",
              "      \"error\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-7bLutQbDbVq0KJ0SGikLnoVR\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1695155870,\n",
              "      \"finished_at\": 1695156183,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::80bqhDsb\",\n",
              "      \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "      \"result_files\": [\n",
              "        \"file-NBHZpOsxP9i16lubc1eBS9CE\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-a6rQQWV5IvYio0esU5q0iRJA\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 5\n",
              "      },\n",
              "      \"trained_tokens\": 6465,\n",
              "      \"error\": null\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.FineTuningJob.list(limit=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RoKaPcTRaBn",
        "outputId": "8f2ccd1f-01d5-49af-be6a-cd487aab8b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7d7e8bdd6c50> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"finished_at\": 1695643682,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::82efbhJ6\",\n",
              "      \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "      \"result_files\": [\n",
              "        \"file-8WfCUkOWa5vW44C9RTOxg4Ug\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 5\n",
              "      },\n",
              "      \"trained_tokens\": 6465,\n",
              "      \"error\": null\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job\",\n",
              "      \"id\": \"ftjob-7bLutQbDbVq0KJ0SGikLnoVR\",\n",
              "      \"model\": \"gpt-3.5-turbo-0613\",\n",
              "      \"created_at\": 1695155870,\n",
              "      \"finished_at\": 1695156183,\n",
              "      \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal::80bqhDsb\",\n",
              "      \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "      \"result_files\": [\n",
              "        \"file-NBHZpOsxP9i16lubc1eBS9CE\"\n",
              "      ],\n",
              "      \"status\": \"succeeded\",\n",
              "      \"validation_file\": null,\n",
              "      \"training_file\": \"file-a6rQQWV5IvYio0esU5q0iRJA\",\n",
              "      \"hyperparameters\": {\n",
              "        \"n_epochs\": 5\n",
              "      },\n",
              "      \"trained_tokens\": 6465,\n",
              "      \"error\": null\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retrieve the current finetuning job and check for status"
      ],
      "metadata": {
        "id": "c4nGSyb4NdW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the FineTuningJob.retrieve method to fetch the current status\n",
        "# of the fine-tuning job with ID \"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\"\n",
        "openai.FineTuningJob.retrieve(\"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IoZ8z44Kzbq",
        "outputId": "d7dcd2dd-2453-4971-8328-c0e8c7d76059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<FineTuningJob fine_tuning.job id=ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn at 0x7d7e8b3e6890> JSON: {\n",
              "  \"object\": \"fine_tuning.job\",\n",
              "  \"id\": \"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"created_at\": 1695643358,\n",
              "  \"finished_at\": null,\n",
              "  \"fine_tuned_model\": null,\n",
              "  \"organization_id\": \"org-Fw23HNdOBixFvYL3rPmDB7Or\",\n",
              "  \"result_files\": [],\n",
              "  \"status\": \"running\",\n",
              "  \"validation_file\": null,\n",
              "  \"training_file\": \"file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "  \"hyperparameters\": {\n",
              "    \"n_epochs\": 5\n",
              "  },\n",
              "  \"trained_tokens\": null,\n",
              "  \"error\": null\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#API to check for events associated with the job"
      ],
      "metadata": {
        "id": "r9R5oXK6NoaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cancel a job\n",
        "# openai.FineTuningJob.cancel(\"ft-abc123\")\n",
        "\n",
        "# List up to 10 events from a fine-tuning job\n",
        "openai.FineTuningJob.list_events(id=\"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\", limit=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iLdl3ocK575",
        "outputId": "d89c392f-b6f7-42bf-ebab-980d0b15caae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7d7ec0f0d5d0> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-1B9qQC6Nteq4T2BfqZLxKgiR\",\n",
              "      \"created_at\": 1695643362,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tuning job started\",\n",
              "      \"data\": null,\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-1FJtC9eiUvWW9ZPO2qVvU5oh\",\n",
              "      \"created_at\": 1695643360,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Files validated, moving job to queued state\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-HfhhnvQE2MojnrQVw1RtF6XT\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Validating training file: file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-jaEkBsBfiHZ1xnbl1yHpWyFK\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Created fine-tuning job: ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List events associated with a fine-tuning job using OpenAI's API.\n",
        "openai.FineTuningJob.list_events(id=\"ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\", limit=30)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txjaPM1WPTp9",
        "outputId": "f716416c-1c1e-483e-94fc-8752911a70f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<OpenAIObject list at 0x7d7e8aef5210> JSON: {\n",
              "  \"object\": \"list\",\n",
              "  \"data\": [\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-yRpD9ORqA46yqPjQJdVQDJH4\",\n",
              "      \"created_at\": 1695643685,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"The job has successfully completed\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-AmAC95HZdhIazE7kcri4Enlp\",\n",
              "      \"created_at\": 1695643683,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal::82efbhJ6\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-RJXmDbtoBTNvK4OFXg81uFgy\",\n",
              "      \"created_at\": 1695643666,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 91/100: training loss=0.08\",\n",
              "      \"data\": {\n",
              "        \"step\": 91,\n",
              "        \"train_loss\": 0.08288879692554474,\n",
              "        \"train_mean_token_accuracy\": 1.0\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-WLDocetkmDO0IO3QrjaUml1K\",\n",
              "      \"created_at\": 1695643652,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 81/100: training loss=0.67\",\n",
              "      \"data\": {\n",
              "        \"step\": 81,\n",
              "        \"train_loss\": 0.6690651774406433,\n",
              "        \"train_mean_token_accuracy\": 0.7407407164573669\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-g6GAERhA6z6IlbhX7zsGlDfb\",\n",
              "      \"created_at\": 1695643636,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 71/100: training loss=0.19\",\n",
              "      \"data\": {\n",
              "        \"step\": 71,\n",
              "        \"train_loss\": 0.1904364377260208,\n",
              "        \"train_mean_token_accuracy\": 0.9166666865348816\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-qoCWjWRnn5iyUyqmyVxPUM54\",\n",
              "      \"created_at\": 1695643622,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 61/100: training loss=0.67\",\n",
              "      \"data\": {\n",
              "        \"step\": 61,\n",
              "        \"train_loss\": 0.6745049357414246,\n",
              "        \"train_mean_token_accuracy\": 0.800000011920929\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-oAIvPbH5yVoFaNPl7Xy0J16y\",\n",
              "      \"created_at\": 1695643608,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 51/100: training loss=0.43\",\n",
              "      \"data\": {\n",
              "        \"step\": 51,\n",
              "        \"train_loss\": 0.43242478370666504,\n",
              "        \"train_mean_token_accuracy\": 0.8799999952316284\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-97x2xDiCJ1dMhJ4pbVjntR13\",\n",
              "      \"created_at\": 1695643592,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 41/100: training loss=0.35\",\n",
              "      \"data\": {\n",
              "        \"step\": 41,\n",
              "        \"train_loss\": 0.34753113985061646,\n",
              "        \"train_mean_token_accuracy\": 0.931034505367279\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-F5mKlcJYYNIS9Jl2efYlbqok\",\n",
              "      \"created_at\": 1695643577,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 31/100: training loss=0.59\",\n",
              "      \"data\": {\n",
              "        \"step\": 31,\n",
              "        \"train_loss\": 0.5890740752220154,\n",
              "        \"train_mean_token_accuracy\": 0.800000011920929\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-7uBUJiUDyBuiPqUVfknB2IMT\",\n",
              "      \"created_at\": 1695643563,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 21/100: training loss=1.64\",\n",
              "      \"data\": {\n",
              "        \"step\": 21,\n",
              "        \"train_loss\": 1.6383657455444336,\n",
              "        \"train_mean_token_accuracy\": 0.5625\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-GP5NpBXKPD79JwZ8iobuKJng\",\n",
              "      \"created_at\": 1695643547,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 11/100: training loss=2.65\",\n",
              "      \"data\": {\n",
              "        \"step\": 11,\n",
              "        \"train_loss\": 2.6477832794189453,\n",
              "        \"train_mean_token_accuracy\": 0.52173912525177\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-GbFgfuxr2hAKccfUOmwtKpKC\",\n",
              "      \"created_at\": 1695643533,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Step 1/100: training loss=2.29\",\n",
              "      \"data\": {\n",
              "        \"step\": 1,\n",
              "        \"train_loss\": 2.2878222465515137,\n",
              "        \"train_mean_token_accuracy\": 0.6896551847457886\n",
              "      },\n",
              "      \"type\": \"metrics\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-1B9qQC6Nteq4T2BfqZLxKgiR\",\n",
              "      \"created_at\": 1695643362,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Fine-tuning job started\",\n",
              "      \"data\": null,\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-1FJtC9eiUvWW9ZPO2qVvU5oh\",\n",
              "      \"created_at\": 1695643360,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Files validated, moving job to queued state\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-HfhhnvQE2MojnrQVw1RtF6XT\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Validating training file: file-xsa036zTL32wxy5eGSUzjwpE\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    },\n",
              "    {\n",
              "      \"object\": \"fine_tuning.job.event\",\n",
              "      \"id\": \"ftevent-jaEkBsBfiHZ1xnbl1yHpWyFK\",\n",
              "      \"created_at\": 1695643358,\n",
              "      \"level\": \"info\",\n",
              "      \"message\": \"Created fine-tuning job: ftjob-3MKXBj9VV4Q5xtUuJkDz0pEn\",\n",
              "      \"data\": {},\n",
              "      \"type\": \"message\"\n",
              "    }\n",
              "  ],\n",
              "  \"has_more\": false\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete a fine-tuned model (must be an owner of the org the model was created in)\n",
        "# openai.Model.delete(\"ftjob-7bLutQbDbVq0KJ0SGikLnoVR\")"
      ],
      "metadata": {
        "id": "_S90_HwJLB_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = openai.File.download(\"file-8WfCUkOWa5vW44C9RTOxg4Ug\")\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-EFjulzMKnb",
        "outputId": "9ab01495-3f1b-400c-f7d6-2dc1d5f68ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\\n1,2.28782,0.68966,,\\n2,2.88231,0.42308,,\\n3,3.00459,0.55556,,\\n4,4.49071,0.5,,\\n5,4.28464,0.35,,\\n6,4.46689,0.375,,\\n7,3.1264,0.44444,,\\n8,2.31342,0.66667,,\\n9,2.4013,0.46429,,\\n10,2.12376,0.56,,\\n11,2.64778,0.52174,,\\n12,2.03157,0.61538,,\\n13,1.64289,0.60714,,\\n14,1.91289,0.53846,,\\n15,1.70548,0.72727,,\\n16,1.25589,0.59259,,\\n17,1.59485,0.71429,,\\n18,2.11192,0.51852,,\\n19,0.98359,0.66667,,\\n20,1.33656,0.54545,,\\n21,1.63837,0.5625,,\\n22,0.73613,0.75,,\\n23,1.79567,0.55556,,\\n24,0.8778,0.75,,\\n25,0.9519,0.77273,,\\n26,0.8665,0.76923,,\\n27,1.01,0.73077,,\\n28,1.10468,0.7037,,\\n29,0.68648,0.83333,,\\n30,0.60373,0.82759,,\\n31,0.58907,0.8,,\\n32,1.55252,0.5,,\\n33,1.13341,0.77778,,\\n34,0.70665,0.80769,,\\n35,0.85422,0.67857,,\\n36,0.76846,0.73913,,\\n37,0.67244,0.68,,\\n38,0.81952,0.77273,,\\n39,0.5078,0.77778,,\\n40,1.09633,0.65,,\\n41,0.34753,0.93103,,\\n42,0.45934,0.92857,,\\n43,0.6221,0.86957,,\\n44,0.3977,0.88889,,\\n45,0.99037,0.77778,,\\n46,0.58957,0.75,,\\n47,0.30539,0.91667,,\\n48,0.45598,0.80769,,\\n49,0.53696,0.84615,,\\n50,0.45871,0.90909,,\\n51,0.43242,0.88,,\\n52,0.8154,0.7,,\\n53,1.40494,0.625,,\\n54,1.12826,0.74074,,\\n55,0.64584,0.85185,,\\n56,0.71959,0.77273,,\\n57,0.35696,0.89286,,\\n58,0.46873,0.76923,,\\n59,0.27015,0.93333,,\\n60,0.95925,0.6875,,\\n61,0.6745,0.8,,\\n62,0.91351,0.74074,,\\n63,0.35896,0.88462,,\\n64,0.32875,0.92308,,\\n65,0.23775,0.92857,,\\n66,0.26839,0.95455,,\\n67,0.18028,0.96667,,\\n68,0.25815,0.89286,,\\n69,0.51326,0.92593,,\\n70,0.71224,0.81481,,\\n71,0.19044,0.91667,,\\n72,0.32181,0.95652,,\\n73,0.29699,0.92,,\\n74,0.3316,0.92857,,\\n75,0.69192,0.8125,,\\n76,0.17473,0.96552,,\\n77,1.09109,0.75,,\\n78,0.27201,0.96154,,\\n79,0.13843,1.0,,\\n80,0.47931,0.81818,,\\n81,0.66907,0.74074,,\\n82,1.00538,0.75,,\\n83,0.27296,0.96429,,\\n84,0.18198,1.0,,\\n85,0.13488,1.0,,\\n86,0.4218,0.86364,,\\n87,0.22189,0.96154,,\\n88,0.13337,0.96429,,\\n89,0.51097,0.9,,\\n90,0.12786,1.0,,\\n91,0.08289,1.0,,\\n92,0.21472,0.96,,\\n93,0.09803,1.0,,\\n94,0.50999,0.875,,\\n95,0.15566,0.92857,,\\n96,0.35668,0.92593,,\\n97,0.22032,0.96154,,\\n98,0.15413,1.0,,\\n99,0.61057,0.81481,,\\n100,0.21473,0.95652,,\\n'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tabulate import tabulate  # You may need to install the 'tabulate' library\n",
        "\n",
        "# Replace the string with your actual output\n",
        "output_bytes = content\n",
        "\n",
        "# Decode the bytes into a string\n",
        "output_string = output_bytes.decode('utf-8')\n",
        "\n",
        "# Remove the leading 'b' and convert to a string\n",
        "output_string = output_string[2:].strip()\n",
        "\n",
        "# Split the string into lines and parse as CSV\n",
        "lines = output_string.split('\\n')\n",
        "reader = csv.reader(lines)\n",
        "\n",
        "# Convert CSV data to a list of lists\n",
        "data = [row for row in reader]\n",
        "\n",
        "# Print the data as a table\n",
        "print(tabulate(data, headers=\"firstrow\", tablefmt=\"grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMppWilPMexj",
        "outputId": "c09c0670-066e-4b83-d694-c380a6c5500c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   ep |   train_loss |   train_accuracy | valid_loss   | valid_mean_token_accuracy   |\n",
            "+======+==============+==================+==============+=============================+\n",
            "|    1 |      2.28782 |          0.68966 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    2 |      2.88231 |          0.42308 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    3 |      3.00459 |          0.55556 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    4 |      4.49071 |          0.5     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    5 |      4.28464 |          0.35    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    6 |      4.46689 |          0.375   |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    7 |      3.1264  |          0.44444 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    8 |      2.31342 |          0.66667 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|    9 |      2.4013  |          0.46429 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   10 |      2.12376 |          0.56    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   11 |      2.64778 |          0.52174 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   12 |      2.03157 |          0.61538 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   13 |      1.64289 |          0.60714 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   14 |      1.91289 |          0.53846 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   15 |      1.70548 |          0.72727 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   16 |      1.25589 |          0.59259 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   17 |      1.59485 |          0.71429 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   18 |      2.11192 |          0.51852 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   19 |      0.98359 |          0.66667 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   20 |      1.33656 |          0.54545 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   21 |      1.63837 |          0.5625  |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   22 |      0.73613 |          0.75    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   23 |      1.79567 |          0.55556 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   24 |      0.8778  |          0.75    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   25 |      0.9519  |          0.77273 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   26 |      0.8665  |          0.76923 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   27 |      1.01    |          0.73077 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   28 |      1.10468 |          0.7037  |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   29 |      0.68648 |          0.83333 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   30 |      0.60373 |          0.82759 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   31 |      0.58907 |          0.8     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   32 |      1.55252 |          0.5     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   33 |      1.13341 |          0.77778 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   34 |      0.70665 |          0.80769 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   35 |      0.85422 |          0.67857 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   36 |      0.76846 |          0.73913 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   37 |      0.67244 |          0.68    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   38 |      0.81952 |          0.77273 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   39 |      0.5078  |          0.77778 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   40 |      1.09633 |          0.65    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   41 |      0.34753 |          0.93103 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   42 |      0.45934 |          0.92857 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   43 |      0.6221  |          0.86957 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   44 |      0.3977  |          0.88889 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   45 |      0.99037 |          0.77778 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   46 |      0.58957 |          0.75    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   47 |      0.30539 |          0.91667 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   48 |      0.45598 |          0.80769 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   49 |      0.53696 |          0.84615 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   50 |      0.45871 |          0.90909 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   51 |      0.43242 |          0.88    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   52 |      0.8154  |          0.7     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   53 |      1.40494 |          0.625   |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   54 |      1.12826 |          0.74074 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   55 |      0.64584 |          0.85185 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   56 |      0.71959 |          0.77273 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   57 |      0.35696 |          0.89286 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   58 |      0.46873 |          0.76923 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   59 |      0.27015 |          0.93333 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   60 |      0.95925 |          0.6875  |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   61 |      0.6745  |          0.8     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   62 |      0.91351 |          0.74074 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   63 |      0.35896 |          0.88462 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   64 |      0.32875 |          0.92308 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   65 |      0.23775 |          0.92857 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   66 |      0.26839 |          0.95455 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   67 |      0.18028 |          0.96667 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   68 |      0.25815 |          0.89286 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   69 |      0.51326 |          0.92593 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   70 |      0.71224 |          0.81481 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   71 |      0.19044 |          0.91667 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   72 |      0.32181 |          0.95652 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   73 |      0.29699 |          0.92    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   74 |      0.3316  |          0.92857 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   75 |      0.69192 |          0.8125  |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   76 |      0.17473 |          0.96552 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   77 |      1.09109 |          0.75    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   78 |      0.27201 |          0.96154 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   79 |      0.13843 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   80 |      0.47931 |          0.81818 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   81 |      0.66907 |          0.74074 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   82 |      1.00538 |          0.75    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   83 |      0.27296 |          0.96429 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   84 |      0.18198 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   85 |      0.13488 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   86 |      0.4218  |          0.86364 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   87 |      0.22189 |          0.96154 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   88 |      0.13337 |          0.96429 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   89 |      0.51097 |          0.9     |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   90 |      0.12786 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   91 |      0.08289 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   92 |      0.21472 |          0.96    |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   93 |      0.09803 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   94 |      0.50999 |          0.875   |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   95 |      0.15566 |          0.92857 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   96 |      0.35668 |          0.92593 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   97 |      0.22032 |          0.96154 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   98 |      0.15413 |          1       |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|   99 |      0.61057 |          0.81481 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n",
            "|  100 |      0.21473 |          0.95652 |              |                             |\n",
            "+------+--------------+------------------+--------------+-----------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.ChatCompletion.create(\n",
        "  model=\"ft:gpt-3.5-turbo-0613:personal::82efbhJ6\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a Software Engineering Mentor specialized in architecture and high-scale systems\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the role of a load balancer in a high-scale system, and which load balancing algorithms are commonly used?\"},\n",
        "    {\"role\": \"assistant\", \"content\":\"\"}\n",
        "  ]\n",
        ")\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhr_1vAhOnnQ",
        "outputId": "99657f98-d402-495c-ea70-f589a11d2ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"A load balancer distributes incoming traffic across multiple servers to ensure availability and optimize resource utilization. Common algorithms include round-robin and least connections.\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the delete operation for the specified fine-tuned model\n",
        "# Note: You must be an owner of the organization under which the model was created to perform this action\n",
        "openai.Model.delete(\"ft:gpt-3.5-turbo-0613:personal::82efbhJ6\")\n"
      ],
      "metadata": {
        "id": "49K8mvw3NIpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hQXFE9VNxxX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}