{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport logging\nimport pandas as pd\nimport transformers as tr\nfrom datasets import load_dataset\nimport sys\nimport json\n\n# Initialize logging\nlogging.basicConfig(level=logging.INFO)\n\n# Create a stream handler and set it to display log messages on the console\nconsole_handler = logging.StreamHandler(sys.stdout)\nlogging.getLogger().addHandler(console_handler)\n\n\n\ndef setup_environment():\n    \"\"\"\n    Sets up the environment by installing necessary packages and libraries.\n    \"\"\"\n    try:\n        # Update package lists and fix broken dependencies\n        logging.info(\"Updating package lists and fixing broken dependencies...\")\n        os.system('sudo apt update && sudo apt-get update > /dev/null 2>&1 && sudo apt --fix-broken install > /dev/null 2>&1')\n        \n        # Create directory for CUDA libraries\n        logging.info(\"Creating directory for CUDA libraries...\")\n        os.system('sudo mkdir -p /tmp/externals/cuda > /dev/null 2>&1')\n\n        # Download and install CUDA libraries\n        logging.info(\"Downloading and installing CUDA libraries...\")\n        cuda_libs = [\n            'libcurand-dev-11-7_10.2.10.50-1_amd64.deb',\n            'libcusparse-dev-11-7_11.7.3.50-1_amd64.deb',\n            'libcublas-dev-11-7_11.10.1.25-1_amd64.deb',\n            'libcusolver-dev-11-7_11.4.0.1-1_amd64.deb'\n        ]\n        for lib in cuda_libs:\n            os.system(f'sudo wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/{lib} \\\n            -P /tmp/externals/cuda > /dev/null 2>&1')\n            os.system(f'sudo dpkg -i /tmp/externals/cuda/{lib} > /dev/null 2>&1')\n\n        # Fix broken dependencies\n        logging.info(\"Fixing any broken dependencies...\")\n        os.system('sudo apt --fix-broken install -y')\n\n        # Install Python libraries\n        logging.info(\"Installing Python libraries...\")\n        os.system('pip install torch transformers datasets deepspeed==0.9.3 py-cpuinfo==9.0.0 tensorboardX pandas ipywidgets accelerate --quiet')\n        os.environ[\"MASTER_ADDR\"] = \"localhost\"\n        os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n        os.environ[\"RANK\"] = \"0\"\n        os.environ[\"LOCAL_RANK\"] = \"0\"\n        os.environ[\"WORLD_SIZE\"] = \"1\"\n        \n    except Exception as e:\n        logging.error(f\"An error occurred during environment setup: {e}\")\n\ndef load_and_tokenize_data(model_checkpoint):\n    \"\"\"\n    Loads and tokenizes the IMDB dataset.\n    \n    Parameters:\n        model_checkpoint (str): The model checkpoint name.\n        \n    Returns:\n        tokenized_dataset: The tokenized dataset.\n        \n    \"\"\"\n    global tokenizer \n    try:\n        logging.info(\"Loading and tokenizing data...\")\n        # Load dataset and tokenizer\n        imdb_ds = load_dataset(\"imdb\")\n        tokenizer = tr.AutoTokenizer.from_pretrained(model_checkpoint, cache_dir=os.path.join(local_training_root, \"tokenizer\"))\n        \n        # Tokenization function\n        def to_tokens(tokenizer, label_map):\n            def apply(x):\n                target_labels = [label_map[y] for y in x[\"label\"]]\n                token_res = tokenizer(\n                    x[\"text\"],\n                    text_target=target_labels,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    padding=True,\n                )\n                return {\n                    \"input_ids\": token_res[\"input_ids\"].tolist(),\n                    \"attention_mask\": token_res[\"attention_mask\"].tolist(),\n                    \"labels\": token_res[\"labels\"].tolist(),\n                }\n            return apply\n\n        imdb_label_lookup = {0: \"negative\", 1: \"positive\", -1: \"unknown\"}\n        imdb_to_tokens = to_tokens(tokenizer, imdb_label_lookup)\n        return imdb_ds.map(imdb_to_tokens, batched=True, remove_columns=[\"text\", \"label\"])\n        \n    except Exception as e:\n        logging.error(f\"An error occurred during data loading and tokenization: {e}\")\n\ndef train_model(model_checkpoint, tokenized_dataset, local_training_root):\n    \"\"\"\n    Trains the model on the tokenized dataset.\n    \n    Parameters:\n        model_checkpoint (str): The model checkpoint name.\n        tokenized_dataset: The tokenized dataset.\n        local_training_root (str): The local directory for training.\n    \"\"\"\n    try:\n        with open('/kaggle/input/deepspeed-config/deepspeed.json', 'r') as f:\n            deepspeed_config_dict = json.load(f)\n        logging.info(\"Training the model...\")\n        # Configure training arguments\n        training_args = tr.TrainingArguments(\n            os.path.join(local_training_root, \"deepspeed_t5-trainer_args\"),\n            num_train_epochs=1,\n            per_device_train_batch_size=8,\n            deepspeed=deepspeed_config_dict,  # add the deepspeed configuration\n            report_to=[\"tensorboard\"],\n            logging_steps=500,\n            weight_decay=0.0, \n            logging_dir='./logs'\n        )\n\n        # Initialize model and trainer\n        model = tr.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, cache_dir=\"/kaggle/temp/deepspeed_t5-trainer_cache\")\n        data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n        trainer = tr.Trainer(\n            model,\n            training_args,\n            train_dataset=tokenized_dataset[\"train\"],\n            eval_dataset=tokenized_dataset[\"test\"],\n            tokenizer=tokenizer,\n            data_collator=data_collator,\n        )\n\n        # Train and save the model\n        trainer.train()\n        trainer.save_model(os.path.join(local_training_root, \"deepspeed_finetuned_t5_model\"))\n        \n    except Exception as e:\n        logging.error(f\"An error occurred during model training: {e}\")\n\ndef predict_and_display(reviews,model_checkpoint):\n    \"\"\"\n    Makes predictions on sample reviews and displays the results.\n    \n    Parameters:\n        model_checkpoint (str): The model checkpoint name.\n        reviews (list): The list of reviews for prediction.\n    \"\"\"\n    try:\n        logging.info(\"Making predictions...\")\n        # Load fine-tuned model and make predictions\n        fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n        inputs = tokenizer(reviews, return_tensors=\"pt\", truncation=True, padding=True)\n        pred = fine_tuned_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n        # Decode and display results\n        decoded_labels = tokenizer.batch_decode(pred, skip_special_tokens=True)\n        pdf = pd.DataFrame({\"review\": reviews, \"classification\": decoded_labels})\n        display(pdf)\n        \n    except Exception as e:\n        logging.error(f\"An error occurred during prediction: {e}\")\n\nif __name__ == \"__main__\":\n    # Define constants\n    local_training_root = \"/kaggle/temp/\"\n    model_checkpoint = \"t5-small\"\n    \n    # Setup environment\n    setup_environment()\n    \n    # Load and tokenize data\n    tokenized_dataset = load_and_tokenize_data(model_checkpoint)\n    \n    # Train model\n    train_model(model_checkpoint, tokenized_dataset, local_training_root)\n    \n    # Sample reviews for prediction\n    reviews = [\n        \"In 'Whimsical Wonders,' the whimsy truly shines through. This enchanting film weaves a tapestry of magic and imagination, making it a delightful experience for viewers of all ages. The characters are endearing, and the plot unfolds like a beautifully illustrated storybook. A must-watch for those seeking a charming escape into a world of wonder.\",\n        \"'Midnight Mystery' is a gripping thriller that keeps you on the edge of your seat from start to finish. The suspense builds relentlessly, and the plot twists are brilliantly executed. The lead actor delivers a mesmerizing performance, making this film an absolute must-see for fans of the genre.\",\n        \"While 'Galactic Odyssey' boasts stunning visual effects and epic space battles, the storyline gets lost amidst the dazzling spectacle. The characters lack depth, and the dialogue feels forced at times. It's a visually impressive journey, but it leaves you wanting more substance in the narrative.\",\n        \"In 'Whispering Shadows,' the cinematography is breathtaking, capturing the haunting beauty of the remote wilderness. The slow-burning mystery keeps you guessing until the very end, and the performances are top-notch. This is a hidden gem that deserves recognition for its atmospheric storytelling.\"\n    ]\n    \n    # Make predictions and display\n    predict_and_display( reviews,os.path.join(local_training_root, \"deepspeed_finetuned_t5_model\"))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T21:00:20.866318Z","iopub.execute_input":"2023-09-29T21:00:20.866795Z","iopub.status.idle":"2023-09-29T21:05:13.746782Z","shell.execute_reply.started":"2023-09-29T21:00:20.866766Z","shell.execute_reply":"2023-09-29T21:05:13.745854Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Updating package lists and fixing broken dependencies...\n","output_type":"stream"},{"name":"stderr","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"},{"name":"stdout","text":"Get:1 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5023 B]\nGet:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\nGet:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\nGet:4 https://packages.cloud.google.com/apt google-fast-socket InRelease [5015 B]\nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1581 B]\nHit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\nGet:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\nGet:8 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [2851 B]\nGet:9 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [518 kB]\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [517 kB]\nGet:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\nGet:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [998 kB]\nGet:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1144 kB]\nGet:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1016 kB]\nGet:15 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.0 kB]\nGet:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1264 kB]\nGet:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1165 kB]\nGet:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.9 kB]\nGet:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1287 kB]\nGet:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [50.4 kB]\nGet:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [28.1 kB]\nFetched 8440 kB in 2s (3636 kB/s)\nReading package lists...\nBuilding dependency tree...\nReading state information...\n","output_type":"stream"},{"name":"stderr","text":"W: http://packages.cloud.google.com/apt/dists/gcsfuse-focal/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\nW: https://packages.cloud.google.com/apt/dists/google-fast-socket/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n","output_type":"stream"},{"name":"stdout","text":"88 packages can be upgraded. Run 'apt list --upgradable' to see them.\nCreating directory for CUDA libraries...\nDownloading and installing CUDA libraries...\nFixing any broken dependencies...\nReading package lists...","output_type":"stream"},{"name":"stderr","text":"\nWARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n\n","output_type":"stream"},{"name":"stdout","text":"\nBuilding dependency tree...\nReading state information...\nCorrecting dependencies... Done\nThe following additional packages will be installed:\n  cuda-toolkit-11-7-config-common libcublas-11-7 libcurand-11-7\n  libcusolver-11-7 libcusparse-11-7\nThe following NEW packages will be installed:\n  cuda-toolkit-11-7-config-common libcublas-11-7 libcurand-11-7\n  libcusolver-11-7 libcusparse-11-7\n0 upgraded, 5 newly installed, 0 to remove and 91 not upgraded.\n4 not fully installed or removed.\nNeed to get 400 MB of archives.\nAfter this operation, 1190 MB of additional disk space will be used.\nGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-toolkit-11-7-config-common 11.7.99-1 [16.2 kB]\nGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcublas-11-7 11.10.3.66-1 [210 MB]\nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcurand-11-7 10.2.10.91-1 [41.5 MB]\nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusolver-11-7 11.4.0.1-1 [45.6 MB]\nGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libcusparse-11-7 11.7.4.91-1 [103 MB]\n","output_type":"stream"},{"name":"stderr","text":"dpkg-preconfigure: unable to re-open stdin: No such file or directory\n","output_type":"stream"},{"name":"stdout","text":"Fetched 400 MB in 7s (60.6 MB/s)\nSelecting previously unselected package cuda-toolkit-11-7-config-common.\n(Reading database ... 113903 files and directories currently installed.)\nPreparing to unpack .../cuda-toolkit-11-7-config-common_11.7.99-1_all.deb ...\nUnpacking cuda-toolkit-11-7-config-common (11.7.99-1) ...\nSelecting previously unselected package libcublas-11-7.\nPreparing to unpack .../libcublas-11-7_11.10.3.66-1_amd64.deb ...\nUnpacking libcublas-11-7 (11.10.3.66-1) ...\nSelecting previously unselected package libcurand-11-7.\nPreparing to unpack .../libcurand-11-7_10.2.10.91-1_amd64.deb ...\nUnpacking libcurand-11-7 (10.2.10.91-1) ...\nSelecting previously unselected package libcusolver-11-7.\nPreparing to unpack .../libcusolver-11-7_11.4.0.1-1_amd64.deb ...\nUnpacking libcusolver-11-7 (11.4.0.1-1) ...\nSelecting previously unselected package libcusparse-11-7.\nPreparing to unpack .../libcusparse-11-7_11.7.4.91-1_amd64.deb ...\nUnpacking libcusparse-11-7 (11.7.4.91-1) ...\nSetting up cuda-toolkit-11-7-config-common (11.7.99-1) ...\nSetting alternatives\nSetting up libcusparse-11-7 (11.7.4.91-1) ...\nSetting up libcusolver-11-7 (11.4.0.1-1) ...\nSetting up libcublas-11-7 (11.10.3.66-1) ...\nSetting up libcurand-11-7 (10.2.10.91-1) ...\nSetting up libcusparse-dev-11-7 (11.7.3.50-1) ...\nSetting up libcurand-dev-11-7 (10.2.10.50-1) ...\nSetting up libcublas-dev-11-7 (11.10.1.25-1) ...\nSetting up libcusolver-dev-11-7 (11.4.0.1-1) ...\nProcessing triggers for libc-bin (2.35-0ubuntu3.1) ...\nInstalling Python libraries...\nLoading and tokenizing data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40f29ba0f364cefa9e6b29944154d87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7587ea1ab1174f03b4867010417f539a"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f01428c68ff84ec89b5469a13e6daba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d91709ef3a064212b0ae6eb15d458dc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051e5734db66405887d3d634ddf9f401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac141013380243c7998f57f382ab0891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de9920ee8ae4fe1a78556983d41a9cf"}},"metadata":{}},{"name":"stdout","text":"Parameter 'function'=<function load_and_tokenize_data.<locals>.to_tokens.<locals>.apply at 0x7fa2d7084820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f3ef58cf6141a7bd69d34c6196ee72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d55daf71652f48d7b453cecf23296048"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cdd968eb1441e2a076370ce4b7cd14"}},"metadata":{}},{"name":"stdout","text":"An error occurred during model training: [Errno 2] No such file or directory: '/kaggle/input/deepspeed-config/deepspeed.json'\nMaking predictions...\nAn error occurred during prediction: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/temp/deepspeed_finetuned_t5_model'. Use `repo_type` argument if needed.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}