{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install the necessary packages\n%pip install rouge_score==0.1.2 datasets transformers evaluate nltk torchmetrics --quiet\n\n# Import the necessary libraries\nimport torch\nimport gc\nimport pandas as pd\nimport nltk\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration, GPT2LMHeadModel, GPT2Tokenizer\nfrom torchmetrics.functional.text.rouge import rouge_score\n\n# Download the CNN/Daily Mail dataset\nfull_dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\", cache_dir=\"./datasets\")\n\n# Select a small random sample of 100 articles from the CNN/Daily Mail dataset\nsample_size = 100\nsample = (\n    full_dataset[\"train\"]\n    .filter(lambda r: \"CNN\" in r[\"article\"][:25])  # Select articles starting with \"CNN\"\n    .shuffle(seed=42)  # Shuffle the dataset to ensure random selection\n    .select(range(sample_size))  # Select the first 100 articles after shuffling\n)\n\n# Function to create batches from a list\ndef batch_generator(data: list, batch_size: int):\n    \"\"\"\n    Generate batches from a list of data.\n\n    Args:\n        data (list): The input data to create batches from.\n        batch_size (int): The desired batch size.\n\n    Yields:\n        list: A batch of data.\n    \"\"\"\n    s = 0\n    e = s + batch_size\n    while s < len(data):\n        yield data[s:e]  # Yield a batch of data\n        s = e\n        e = min(s + batch_size, len(data))  # Update the start and end indices for the next batch\n\n# Function to compute ROUGE scores\ndef compute_rouge_score(generated: list, reference: list) -> dict:\n    \"\"\"\n    Compute ROUGE scores between generated and reference texts.\n\n    Args:\n        generated (list): List of generated texts.\n        reference (list): List of reference texts.\n\n    Returns:\n        dict: Dictionary containing ROUGE scores.\n    \"\"\"\n    generated_with_newlines = [\"\\n\".join(nltk.sent_tokenize(s.strip())) for s in generated]  # Tokenize the generated summaries\n    reference_with_newlines = [\"\\n\".join(nltk.sent_tokenize(s.strip())) for s in reference]  # Tokenize the reference summaries\n    return rouge_score(\n        generated_with_newlines,\n        reference_with_newlines,\n    )  # Compute the ROUGE scores\n\n# Function to summarize using T5 model\ndef summarize_with_t5(model_checkpoint: str, articles: list, batch_size: int = 8) -> list:\n    \"\"\"\n    Summarize a list of articles using the T5 model.\n\n    Args:\n        model_checkpoint (str): The pre-trained T5 model checkpoint.\n        articles (list): List of articles to summarize.\n        batch_size (int, optional): Batch size for inference. Defaults to 8.\n\n    Returns:\n        list: List of generated summaries.\n    \"\"\"\n    # Set the device for computation\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n    else:\n        device = \"cpu\"\n\n    # Load the pre-trained T5 model and tokenizer\n    model = T5ForConditionalGeneration.from_pretrained(\n        model_checkpoint, cache_dir=\"./datasets\"\n    ).to(device)\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_checkpoint, model_max_length=1024, cache_dir=\"./datasets\"\n    )\n\n    # Function to perform inference using the T5 model\n    def perform_inference(batch: list) -> list:\n        # Prepare the inputs\n        inputs = tokenizer(\n            batch, max_length=1024, return_tensors=\"pt\", padding=True, truncation=True\n        )\n\n        # Generate the summary\n        summary_ids = model.generate(\n            inputs.input_ids.to(device),\n            attention_mask=inputs.attention_mask.to(device),\n            num_beams=2,\n            min_length=0,\n            max_length=40,\n        )\n        return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)  # Decode the generated summary\n\n    # List to store the generated summaries\n    res = []\n\n    # Prepend \"summarize: \" to each article\n    summary_articles = list(map(lambda article: \"summarize: \" + article, articles))\n    for batch in batch_generator(summary_articles, batch_size=batch_size):  # Iterate over batches\n        res += perform_inference(batch)  # Perform inference on each batch and add to the results list\n\n    # Clean up resources\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    del tokenizer\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n    return res  # Return the generated summaries\n\n# Function to summarize using GPT-2 model\ndef summarize_with_gpt2(model_checkpoint: str, articles: list, batch_size: int = 8) -> list:\n    \"\"\"\n    Summarize a list of articles using the GPT-2 model.\n\n    Args:\n        model_checkpoint (str): The pre-trained GPT-2 model checkpoint.\n        articles (list): List of articles to summarize.\n        batch_size (int, optional): Batch size for inference. Defaults to 8.\n\n    Returns:\n        list: List of generated summaries.\n    \"\"\"\n    # Set the device for computation\n    if torch.cuda.is_available():\n        device = \"cuda:0\"\n    else:\n        device = \"cpu\"\n\n    # Load the pre-trained GPT-2 model and tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(\n        model_checkpoint, padding_side=\"left\", cache_dir=\"./datasets\"\n    )\n    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n    model = GPT2LMHeadModel.from_pretrained(\n        model_checkpoint,\n        pad_token_id=tokenizer.eos_token_id,\n        cache_dir=\"./datasets\",\n    ).to(device)\n\n    # Function to perform inference using the GPT-2 model\n    def perform_inference(batch: list) -> list:\n        # Prepare the inputs\n        tmp_inputs = tokenizer(\n            batch, max_length=500, return_tensors=\"pt\", padding=True, truncation=True\n        )\n        tmp_inputs_decoded = tokenizer.batch_decode(\n            tmp_inputs.input_ids, skip_special_tokens=True\n        )\n        inputs = tokenizer(\n            [article + \" TL;DR:\" for article in tmp_inputs_decoded],\n            max_length=512,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n        )\n\n        # Generate the summary\n        summary_ids = model.generate(\n            inputs.input_ids.to(device),\n            attention_mask=inputs.attention_mask.to(device),\n            num_beams=2,\n            min_length=0,\n            max_length=512 + 32,\n        )\n        return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)  # Decode the generated summary\n\n    # List to store the decoded summaries\n    decoded_summaries = []\n    for batch in batch_generator(articles, batch_size=batch_size):  # Iterate over batches\n        decoded_summaries += perform_inference(batch)  # Perform inference on each batch and add to the results list\n\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # Extract the summaries from the decoded summaries\n    summaries = [\n        summary[summary.find(\"TL;DR:\") + len(\"TL;DR: \") :]\n        for summary in decoded_summaries\n    ]\n\n    # Clean up resources\n    del tokenizer\n    del model\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return summaries  # Return the generated summaries\n\n# Generate summaries using T5 model\nt5_small_summaries = summarize_with_t5(\"t5-small\", sample[\"article\"])\n\n# Compute ROUGE scores for T5 model\nt5_rouge_scores = compute_rouge_score(t5_small_summaries, sample[\"highlights\"])\n\n# Generate summaries using GPT-2 model\ngpt2_summaries = summarize_with_gpt2(\"gpt2\", sample[\"article\"])\n\n# Compute ROUGE scores for GPT-2 model\ngpt2_rouge_scores = compute_rouge_score(gpt2_summaries, sample[\"highlights\"])\n\n# Construct a DataFrame to display the results\nresults_df = pd.DataFrame({\n    \"Model\": [\"T5-Small\", \"GPT-2\"],\n    \"ROUGE-1 F1\": [t5_rouge_scores[\"rouge1_fmeasure\"].item(), gpt2_rouge_scores[\"rouge1_fmeasure\"].item()],\n    \"ROUGE-1 Precision\": [t5_rouge_scores[\"rouge1_precision\"].item(), gpt2_rouge_scores[\"rouge1_precision\"].item()],\n    \"ROUGE-1 Recall\": [t5_rouge_scores[\"rouge1_recall\"].item(), gpt2_rouge_scores[\"rouge1_recall\"].item()],\n    \"ROUGE-2 F1\": [t5_rouge_scores[\"rouge2_fmeasure\"].item(), gpt2_rouge_scores[\"rouge2_fmeasure\"].item()],\n    \"ROUGE-2 Precision\": [t5_rouge_scores[\"rouge2_precision\"].item(), gpt2_rouge_scores[\"rouge2_precision\"].item()],\n    \"ROUGE-2 Recall\": [t5_rouge_scores[\"rouge2_recall\"].item(), gpt2_rouge_scores[\"rouge2_recall\"].item()],\n    \"ROUGE-L F1\": [t5_rouge_scores[\"rougeL_fmeasure\"].item(), gpt2_rouge_scores[\"rougeL_fmeasure\"].item()],\n    \"ROUGE-Lsum Precision\": [t5_rouge_scores[\"rougeLsum_precision\"].item(), gpt2_rouge_scores[\"rougeLsum_precision\"].item()],\n    \"ROUGE-Lsum Recall\": [t5_rouge_scores[\"rougeLsum_recall\"].item(), gpt2_rouge_scores[\"rougeLsum_recall\"].item()],\n})\n\n# Display the results DataFrame\nprint(\"ROUGE Scores:\")\ndisplay(results_df)\n                                          \n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T20:19:25.044273Z","iopub.execute_input":"2023-09-29T20:19:25.044630Z","iopub.status.idle":"2023-09-29T20:38:05.416659Z","shell.execute_reply.started":"2023-09-29T20:19:25.044601Z","shell.execute_reply":"2023-09-29T20:38:05.415297Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.51k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d430ee627a43e4bb6e4fe95fe8f33b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68fe499354034f2882dc35a34d210349"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset cnn_dailymail/default to ./datasets/cnn_dailymail/default/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60a05087c172404d94a34aed9b50e150"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9611373d09e54c318513f4b776df96db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06368113cde34dd1b568a1cf554f20e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c43ef9a4224450fb7a989180ce347ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c3c846a48534844948f42e00218b546"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"051c639a491444dd9313893f3426e4f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f5560a6f544680b5168cfd9858c334"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset cnn_dailymail downloaded and prepared to ./datasets/cnn_dailymail/default/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ddc0d1414c4168ada73b74edd6093d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/288 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4331e8ebe56449419073e08c1d45b2d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c564d346219e490d82e68aefb560904f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d176c527acd478b962b5a2414d35c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86ee94d227434d5d813ddb2ac93756d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21c129edd3514fc4aadc33d9471fb0cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2eb69fad95440cb9d830403a6283f0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea15ea1f53be4e24a0b8d2c8de5ca42b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd533d9832544c7ea71502df14559a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59168bbb03b4ed8ab12c5f38f3e4aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b639449dec34bcd9859fb6acc6619c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c15dffe7eb148af80720b0e5bcaa6f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ac980c48ca54b2a9ac6a1a2a051de29"}},"metadata":{}},{"name":"stdout","text":"ROUGE Scores:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      Model  ROUGE-1 F1  ROUGE-1 Precision  ROUGE-1 Recall  ROUGE-2 F1  \\\n0  T5-Small    0.293450           0.390506        0.241019    0.112200   \n1     GPT-2    0.184629           0.259675        0.154533    0.044773   \n\n   ROUGE-2 Precision  ROUGE-2 Recall  ROUGE-L F1  ROUGE-Lsum Precision  \\\n0           0.150521        0.091911    0.224367              0.362700   \n1           0.065962        0.036355    0.144070              0.242341   \n\n   ROUGE-Lsum Recall  \n0           0.222510  \n1           0.141547  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>ROUGE-1 F1</th>\n      <th>ROUGE-1 Precision</th>\n      <th>ROUGE-1 Recall</th>\n      <th>ROUGE-2 F1</th>\n      <th>ROUGE-2 Precision</th>\n      <th>ROUGE-2 Recall</th>\n      <th>ROUGE-L F1</th>\n      <th>ROUGE-Lsum Precision</th>\n      <th>ROUGE-Lsum Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>T5-Small</td>\n      <td>0.293450</td>\n      <td>0.390506</td>\n      <td>0.241019</td>\n      <td>0.112200</td>\n      <td>0.150521</td>\n      <td>0.091911</td>\n      <td>0.224367</td>\n      <td>0.362700</td>\n      <td>0.222510</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GPT-2</td>\n      <td>0.184629</td>\n      <td>0.259675</td>\n      <td>0.154533</td>\n      <td>0.044773</td>\n      <td>0.065962</td>\n      <td>0.036355</td>\n      <td>0.144070</td>\n      <td>0.242341</td>\n      <td>0.141547</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}