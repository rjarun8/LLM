{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Document based QA Bot with Large Language Models"
      ],
      "metadata": {
        "id": "jGqUP3Dg8tCL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ld_vnq8hiK5",
        "outputId": "e2612d27-8e2b-4495-b9bf-46b438c375a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/607.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/607.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m278.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m242.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m246.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m254.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m290.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m226.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m276.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m299.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m269.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m252.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m234.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m242.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m162.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet --no-cache-dir llama-index pinecone-client openai transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import required libraries\n",
        "# ---------------------------------\n",
        "import os\n",
        "from pathlib import Path\n",
        "from llama_index import Document, download_loader, GPTVectorStoreIndex, StorageContext, ServiceContext\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores import PineconeVectorStore\n",
        "import openai\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "yiTtBTf-5w4f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load documents using Llama-Index\n",
        "# -----------------------------------------\n",
        "# A PDFReader is downloaded from Llama-Index to read the contents of the PDF document.\n",
        "PDFReader = download_loader(\"PDFReader\")\n",
        "\n",
        "# The PDFReader is then used to load the PDF document and store the content into the 'documents' variable.\n",
        "loader = PDFReader()\n",
        "documents = loader.load_data(file=Path('/content/blockchain-book.pdf'))"
      ],
      "metadata": {
        "id": "Rk87-7BB73XT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Setup OpenAI environment\n",
        "# --------------------------------\n",
        "# Set up the OpenAI environment using the API key.\n",
        "os.environ['OPENAI_API_KEY'] = \"your_openai_api_key\"\n",
        "openai.api_key = \"your_openai_api_key\""
      ],
      "metadata": {
        "id": "iC-UZk5J8E5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Setup Pinecone environment\n",
        "# ----------------------------------\n",
        "# Set up the Pinecone environment using the API key and environment details.\n",
        "os.environ['PINECONE_API_KEY'] = 'your_pinecone_api_key'\n",
        "os.environ['PINECONE_ENVIRONMENT'] = 'your_pinecone_environment'\n",
        "\n",
        "# Initialize connection to Pinecone\n",
        "pinecone.init(\n",
        "    api_key=os.environ['PINECONE_API_KEY'],\n",
        "    environment=os.environ['PINECONE_ENVIRONMENT']\n",
        ")\n",
        "\n",
        "# Create an index in Pinecone if it does not exist already. This index will store the document embeddings.\n",
        "index_name = 'docindex'\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,\n",
        "        metric='cosine'\n",
        "    )\n",
        "\n",
        "# Connect to the created index in Pinecone\n",
        "pinecone_index = pinecone.Index(index_name)"
      ],
      "metadata": {
        "id": "8XNisuGx73eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Setup Vector Store with Pinecone\n",
        "# ----------------------------------------\n",
        "# The Pinecone index is used to create a PineconeVectorStore, which will store the document embeddings.\n",
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
      ],
      "metadata": {
        "id": "oCUb5Edp8PWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Setup Storage and Service Context\n",
        "# ------------------------------------------\n",
        "# The StorageContext is set up using the vector store. It specifies where the document embeddings will be stored.\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store\n",
        ")\n",
        "\n",
        "# The ServiceContext is set up using OpenAI's text embedding model. It specifies how the documents will be embedded.\n",
        "embed_model = OpenAIEmbedding(model='text-embedding-ada-002', embed_batch_size=100)\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model)\n"
      ],
      "metadata": {
        "id": "mzKpyZaY8MP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Indexing documents\n",
        "# --------------------------\n",
        "# The documents are indexed using the GPTVectorStoreIndex. This involves embedding the documents using the embed_model\n",
        "# specified in the ServiceContext, and storing the embeddings in the vector store specified in the StorageContext.\n",
        "index = GPTVectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context,\n",
        "    service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "gmeXKWxt8TLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Querying the index\n",
        "# ---------------------------\n",
        "# The index can now be used to answer queries. The queries are embedded using the same embed_model, and the embeddings\n",
        "# are compared to the document embeddings in the vector store to find the most similar documents.\n",
        "query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "t_ss-CZ18TOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "res = query_engine.query(\"What is consensus in blockchain\")\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "wB5QDsHtm_dd",
        "outputId": "1986c376-dfed-4350-8c22-6b989c485a9a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consensus in blockchain is the process of reaching agreement among all participants in a network on the validity of a transaction. It is achieved through a variety of consensus algorithms, such as Dev-Mode, PoET (Proof of Elapsed Time), and others. These algorithms are used to validate transactions and ensure that all participants in the network agree on the validity of the transaction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = query_engine.query(\"can you explain proof of work and proof of stake\")\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAKDdfGfCEIa",
        "outputId": "edcd38d0-933e-4469-8acb-9949d5a27afb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Proof of Work (PoW) is a consensus algorithm used by blockchain networks to validate transactions and create new blocks. It is an open challenge to all miners to solve a complex mathematical puzzle. The miner who solves the puzzle first is rewarded with a fixed amount of coins. PoW requires a lot of computational power and is used by Bitcoin and other cryptocurrencies.\n",
            "\n",
            "Proof of Stake (PoS) is an alternative consensus algorithm to PoW. In PoS, the validators are chosen based on the fraction of coins they own in the system. The nodes with more number of coins have more chance to be selected than the node with lesser number of coins. In PoS the reward is in the form of transaction fee, new coins are not created for paying the validators. Presently, Blackcoin, NXT and Peercoin blockchains uses the PoS algorithm. Ethereum is also planning to shift to this method by 2018.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FqZWSGO-8V08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Z9Wy9zD8V8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Web based QA Bot with Large Language Models"
      ],
      "metadata": {
        "id": "o-0R6raJ8x0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain cohere faiss-cpu  loguru unstructured sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqeAIpKH963",
        "outputId": "e8dd44dc-ed39-48b9-b507-f5ecca99534f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from typing import Optional\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.llms import Cohere\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "import requests\n",
        "from loguru import logger\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "class WebsiteQABot:\n",
        "    def __init__(\n",
        "        self,\n",
        "        urls: list,\n",
        "        chunk_size: int,\n",
        "        chunk_overlap: int,\n",
        "    ):\n",
        "        # Initialize the bot and start the logging\n",
        "        logger.info(\"Building the QA Bot ...\")\n",
        "\n",
        "        # Load the URLs content\n",
        "        logger.info(\"Loading URLs content ...\")\n",
        "        loader = UnstructuredURLLoader(urls)\n",
        "        data = loader.load()\n",
        "\n",
        "        # Split the documents into chunks of a specific size\n",
        "        logger.info(\"Splitting documents in chunks ...\")\n",
        "        doc_splitter = CharacterTextSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        docs = doc_splitter.split_documents(data)\n",
        "        logger.info(\"{n} chunks created\", n=len(docs))\n",
        "\n",
        "        # Build the vector database using the HuggingFaceInstructEmbeddings\n",
        "        logger.info(\"Building the vector database ...\")\n",
        "        embeddings = HuggingFaceEmbeddings()\n",
        "        docsearch = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "        # Save the database to a pickle file\n",
        "        with open(\"faiss_store_instruct.pkl\", \"wb\") as f:\n",
        "            pickle.dump(docsearch, f)\n",
        "\n",
        "        # Create a retriever that retrieves the top 3 most relevant documents\n",
        "        retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "        # Build the retrieval chain using the Cohere model\n",
        "        logger.info(\"Building the retrieval chain ...\")\n",
        "        os.environ[\"COHERE_API_KEY\"] = \"\"\n",
        "        llm = Cohere(model=\"command-nightly\", temperature=0)\n",
        "        self.chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=retriever)\n",
        "\n",
        "        logger.info(\"QA Bot created!\")\n",
        "\n",
        "    # Function to ask a query to the bot\n",
        "    def ask(self, query: str):\n",
        "        return self.chain({\"question\": query}, return_only_outputs=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the URLs from where the data will be loaded\n",
        "    urls = [\n",
        "        \"https://blog.monsterapi.ai/what-is-falcon-7b-instruct-a-better-alternative-to-gpt-3/\"\n",
        "    ]\n",
        "    # Create the bot\n",
        "    bot = WebsiteQABot(\n",
        "        urls=urls,\n",
        "        chunk_size=2000,\n",
        "        chunk_overlap=100,\n",
        "    )\n",
        "\n",
        "    # Ask a query to the bot\n",
        "    query =\"why Is Falcon-7B Instruct a better alternative to GPT-3?\"\n",
        "    res = bot.ask(query)\n",
        "    print(res)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jf0wozUGEJN",
        "outputId": "ef187e22-930a-406e-fa79-6d68fe8c8e83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2023-07-21 11:31:06.944\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mBuilding the QA Bot ...\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:06.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mLoading URLs content ...\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:07.594\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mSplitting documents in chunks ...\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:07.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1m4 chunks created\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:07.602\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mBuilding the vector database ...\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:18.374\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mBuilding the retrieval chain ...\u001b[0m\n",
            "\u001b[32m2023-07-21 11:31:18.377\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mQA Bot created!\u001b[0m\n",
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
            "WARNING:langchain.llms.cohere:Retrying langchain.llms.cohere.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised CohereAPIError: You are using a Trial key, which is limited to 5 API calls / minute. You can continue to use the Trial key for free or upgrade to a Production key with higher rate limits at 'https://dashboard.cohere.ai/api-keys'. Contact us on 'https://discord.gg/XW44jPfYJu' or email us at support@cohere.com with any questions.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1949 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'question': 'why Is Falcon-7B Instruct a better alternative to GPT-3?', 'answer': ' Falcon-7B Instruct has been fine-tuned on instructions and conversational data, making it more suitable for popular assistant-style tasks. It requires less GPU memory, making it more accessible on consumer hardware. It offers a unique combination of affordability and performance, making it a compelling alternative to GPT-3.', 'sources': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WczmNlL6JIbn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}